\chapter{Einleitung}
\label{Einleitung}

Das heutige World Wide Web (kurz: Web) besitzt aufgrund seiner rasanten Entwicklung eine für Menschen unüberschaubare, stetig wachsende Menge an Informationen. Neben den Vorteilen der universellen Verfügbarkeit, der ständigen Aktualität und der einfachen Bereitstellung von Daten überwiegt jedoch ein wesentlicher Nachteil: die Ausrichtung der Informationen orientiert sich an einem menschlichen Betrachter. Im Gegensatz zu einer Maschine kann ein menschlicher Benutzer die Bedeutung von Informationen auf einer Webseite erfassen, zueinander in Beziehung setzen und in eine andere Darstellungsform transformieren, um neues Wissen zu generieren. Die Datenfülle und dezentrale Organisation des Webs führen dazu, dass eine Suche nach gewünschten Informationen erschwert wird. Erstrebenswert ist daher eine inhaltliche und semantische Suche.

Hierzu prägte Tim Berners-Lee 2001 das Konzept Semantic Web. Die Grundidee des Konzepts besteht darin, das herkömmliche Web durch Standards zu erweitern. Im Vordergrund steht dabei die Verwertung der Informationen von Maschinen und der einfache Austausch zwischen diesen. Dazu sind grundlegende Anforderungen notwendig, die neben einer klaren Definition des Standards auch eine flexible Anwendung und mögliche Erweiterungen erlauben sollen.

Aus diesem Grund wurde das Resource Description Framework (RDF) entwickelt. RDF ist eine Modellierungssprache zur Beschreibung strukturierter Informationen und ein Standard des World Wide Web Consortium. Mit dem RDF-Standard soll es Anwendungen ermöglicht werden, Daten über das Web auszutauschen, ohne ihre eigentliche Bedeutung zu verlieren und sie mit zusätzlichen Informationen aus anderen Datenquellen anzureichern. Im Gegensatz zum herkömmlichen Web geht es hierbei nicht nur um eine korrekte Darstellung von Daten oder Dokumenten, sondern vielmehr um die Verknüpfung von Daten aus unterschiedlichen Quellen für die Weiterverarbeitung der Informationen.

\newpage

\section{Motivation}
\label{Motivation}

In den letzten Jahren ist das Interesse gestiegen, statistische Daten nach dem Linked-Data-Prinzip zu veröffentlichen und die Möglichkeit zu bieten, Daten mit anderen Informationen aus unterschiedlichen Quellen zu kombinieren. Ein Vorteil besteht darin, beliebige Zusatzinformationen zu den statistischen Daten zu verlinken, um die Bedeutung der Daten näher zu bestimmen und neue Erkenntnisse zu erlangen. Beispielsweise können Provenance-Informationen (z.\,B. Woher stammen die Daten? Welche Qualität besitzen sie?) oder weitergehende Informationen (z.\,B. Welche Anzahl an Mitarbeitern besitzt die Firma, von denen die Daten handeln?) hinzugefügt werden. Ferner können auch interne Daten (z.\,B. aus dem Intranet) einer Firma mit den statistischen Daten verlinkt und zur Analyse verwendet werden.

Bevor Analysten jedoch in der Lage sind, Unternehmensleistungen vergleichen zu können, verbringen sie unverhältnismäßig viel Zeit mit der Identifizierung, Erfassung und Aufbereitung der relevanten Daten. Der Aufwand steigt mit der Anzahl heterogener Datenquellen. Damit verbunden sind unterschiedliche Formate oder Bezeichnungen für identische Objekte. Diese Prozesse müssen optimiert und möglichst automatisiert werden. Zusätzlich führt die zunehmende Größe an verfügbaren RDF-Datensätzen\footnote{ s. Datensätze, die nach dem Linked-Data-Prinzip veröffentlicht werden: \url{http://lod-cloud.net/}.} dazu, dass diese nicht mehr effizient auf einem einzelnen Rechner verarbeitet und analysiert werden können. Daher sind neue Konzepte zur Auswertung statistischer Datensätze notwendig. Ferner ist ein Lösungsansatz erforderlich, der eine beliebig große RDF-Datenmenge generisch und automatisiert integriert, diese in geeigneter Form aufbereitet und dem Nutzer präsentiert.

\section{Zielsetzung}
\label{Zielsetzung}

Für entscheidungsunterstützende Analysen numerischer Datensätze bietet das Konzept OLAP (\textbf{O}n-\textbf{L}ine \textbf{A}nalytical \textbf{P}rocessing) eine multidimensionale Betrachtung des Datenbestands. In einer vorangegangenen Arbeit von Kämpgen und Harth \cite{kampgen2011transforming} wurde hierfür ein Extract-, Transform- und Load-Prozess (ETL-Prozess) vorgestellt, der statistische Linked Data aus unterschiedlichen RDF Stores, unter Anwendung der Abfragesprache \mbox{SPARQL} und dem RDF Data Cube Vocabulary (QB), in ein multidimensionales Datenmodell transformiert. Die Daten wurden für die Analysen in einem relationalen Data Warehouse gespeichert. Auf diese Weise konnte mit der OLAP-to-SQL-Engine Mondrian die Vorteile der multidimensionalen Abfragemöglichkeit und erweiterten Selektierbarkeit von OLAP-Anfragen mit MDX (engl. für \textbf{M}ulti\textbf{D}imensional E\textbf{X}pression) genutzt werden. Ziel des ETL-Prozesses war es, entscheidungsunterstützende Analysen auf RDF-Datensätzen durchzuführen. Dieser Ansatz beinhaltet jedoch vier wesentliche Probleme:

\begin{compactenum}[(V1)]
\item Die Dauer des ETL-Prozesses bei gro{\ss}en Datensätzen mit vielen Zusatzinformationen ist nicht zufriedenstellend, da innerhalb der RDF-Daten die nötigen Informationen für das multidimensionale Datenmodell (Metadaten und Daten) herausgezogen werden müssen.
\item Bei einer Aktualisierung des Datenbestands muss der ETL-Prozess neu durchgeführt werden.
\item Bei der Hinzunahme neuer Daten muss der ETL-Prozess ebenfalls neu durchgeführt werden.
\item Zusatzinformationen in den Datensätzen werden bei der Erstellung des multidimensionalen Datenmodells gefiltert und können in den analytischen Abfragen nicht berücksichtigt oder als Zusatzinformation abgefragt werden.
\end{compactenum}

Sowohl relationale Datenbanken, RDF Stores als auch OLAP Engines skalieren in der Regel nicht horizontal und besitzen daher eine natürliche Grenze bzgl. ihrer Datenspeicher- und Datenverarbeitungskapazität. Aus diesem Grund sind für Analysen gro{\ss}er Datenmengen Technologien aus dem Big-Data-Umfeld notwendig, die diese Beschränkungen mittels Parallelisierung über viele Rechner hinweg überwinden. Mit Apache Hadoop sind derartige Technologien in einem Open Source Software Stack verfügbar. Bislang wurde nicht erforscht, ob eine enorm große RDF-Datenmenge in einem automatisierten ETL-Prozess durch eine Umsetzung der Architektur von Kämpgen und Harth mit Komponenten aus dem Hadoop-Ökosystem für OLAP-Analysen bereitgestellt werden kann.

Der hier präsentierte Lösungsansatz überführt Kämpgen und Harths Konzept in eine horizontal skalierende Architektur auf der Basis von Apache Hadoop. Die nicht-skalierbaren Komponenten, wie die RDF-Datenbank, die Abfragesprache \mbox{SPARQL} und die relationale Datenbank, werden dabei durch Technologien und Frameworks aus dem Hadoop-Ökosystem ersetzt.

Eine weitere Zielsetzung dieser Arbeit liegt in der generischen und automatisierten Umsetzung des ETL-Prozesses unter Verwendung der Metainformationen des QB-Vokabulars. Ferner soll der ETL-Prozess auf seine Ausführungszeit im Hinblick auf die Anzahl der verwendeten Rechner im Vergleich zur relationalen Datenbank MySQL und dem RDF Store Open Virtuoso evaluiert werden.

\section{Verwandte Arbeiten}
\label{Verwandte-Arbeiten}

In einer vorherigen Arbeit \cite{NoSize} wurden die statistischen Daten in einem RDF Store geladen, um analytische Abfragen mittels der graphenbasierten Sprache SPARQL auszuführen. Es zeigte sich, dass die Ausführung der analytischen Abfragen weniger effizient durchgeführt wurden, als vergleichsweise eine relationale Datenbank mit äquivalenten Daten im Sternschema. Eine weitere Arbeit~\cite{NoSQL} beschäftigte sich mit der Optimierung eines RDF Stores durch horizontale Skalierung. Da NoSQL-Systeme für komplexe OLAP-Operationen weniger geeignet sind, war die Ausführung der analytischen Abfragen nicht effizient genug. In der Arbeit von Abelló et al.~\cite{Abello} wurden analytische Abfragen auf MapReduce-basierten Systemen evaluiert. Dabei wurden die Vorteile von Big-Data-Technologien bei der Generierung eines OLAP Cubes für analytische Abfragen überprüft, jedoch ohne eine horizontale skalierbare OLAP Engine für die Analysen zu verwenden. Zusammenfassend kann festgestellt werden, dass eine Analyse einer beliebig großen Menge an RDF-Daten eine Herausforderung darstellt.

\section{Gliederung}
\label{Gliederung}

Die vorliegende Arbeit ist in sechs Kapitel unterteilt. Das erste Kapitel gibt dem interessierten Leser eine kurze Einführung und einen Gesamtüberblick über die geplanten Ziele. Das zweite Kapitel enthält eine ausführliche Beschreibung der Grundlagen und der verwendeten Begriffe, wobei der Fokus auf traditionellen Konzepten für Analysen statistischer Datensätze, der Idee hinter dem Semantic Web und auf die in dieser Arbeit verwendeten Big-Data-Technologien im Verbund mit Apache Hadoop und sein Ökosystem liegt. Der dritte Abschnitt beschreibt das in dieser Arbeit entworfene Konzept und die Gesamtarchitektur. Zudem werden die verwendeten Komponenten aus dem Apache Hadoop-Ökosystem sowie ihr Zusammenspiel in diesem Abschnitt einer näheren Betrachtung unterzogen. Das vierte Kapitel beschreibt die technische Umsetzung des ETL-Prozesses. Auf den Evaluationsaufbau und die erhaltenen Ergebnisse wird im fünften Kapitel eingegangen. Anschließend werden im letzten Kapitel das Fazit der Arbeit und mögliche Erweiterungen besprochen.
